# -*- coding: utf-8 -*-
"""Final_of_TaskB_sharedtask_baseline_with_lemmatization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jFLkvKoku47BPAblH-KFGGK-uQn6dK7D
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plot
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import svm
from nltk.tokenize import word_tokenize
from nltk.stem.snowball import SnowballStemmer
from sklearn.model_selection import StratifiedShuffleSplit, train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.metrics import classification_report
from sklearn.metrics import plot_confusion_matrix

import argparse
# import these modules
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')



def create_arg_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_file", default='./train_all_tasks.csv', type=str,
                        help="File containing the reviews, which are split up later")
    parser.add_argument("--task_type", type=str, default="A",
                        help="A or B")
    parser.add_argument("--vectorizer", type=str, default="tfidf",
                        help="Type of vectorizer: tfidf or tf (just term frequency)")
    parser.add_argument("--seed", type=int, default=120,
                        help="Seed for random state for experiments")
    args = parser.parse_args()
    return args


def preprocess(df):
  """<!-- Clean and prepare the training data -->"""
  # lowercase
  df['text'] = [entry.lower() for entry in df['text']]

  # remove urls
  df['text'] = df['text'].str.replace(r'http\S+', "", regex=True)

  # removal of non-word characters, interpunction, other non-alphanumeric chars
  df['text'] = df['text'].str.replace(r"[^\w\s]", "", regex=True)

  # tokenization
  df['tokenized_text'] = [word_tokenize(entry) for entry in df['text']]

  # stemming / lemmatization
  def lemmatize_text(text):
    return " ".join([lemmatizer.lemmatize(w) for w in word_tokenize(text)])

  df["lemmatized_text"] = df.text.apply(lemmatize_text)

  # stemmer = SnowballStemmer("english")

  # # stemming / lemmatization
  # def lemmatize_text(text):
  #   return " ".join([stemmer.stem(w) for w in word_tokenize(text)])
  return df

def filter_none_class(reviews, labels):
  new_list_text = []
  new_list_label = []
  for review, lbl in zip(reviews, labels):
    if lbl != "none":
      new_list_text.append(review)
      new_list_label.append(lbl)
  
  return new_list_text, new_list_label

def split_data(df, seed):
  """Create train, dev, test sets with representative proportions

  9800 training examples

  2100 development examples

  2100 test examples
  """

  X = df["lemmatized_text"]
  y_taskA = df["label_sexist"]
  y_taskB = df["label_category"]
  y_taskC = df["label_vector"]

  if args.task_type == "A":
    y = y_taskA
  elif args.task_type == "B":
    y = y_taskB
  else:
    assert 1!=1, "Wrong task type chosen. Enter A/B only"

  X, y = filter_none_class(X, y)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)

  # Split test set again for dev/test split
  X_test, X_dev, y_test, y_dev = train_test_split(X_test, y_test, test_size=0.5, random_state=seed)


  return X_train, X_dev, X_test, y_train, y_dev, y_test


def get_vectorizer(args):
  """Select a Vectorizer for converting the data into matrices"""
  tfidf = TfidfVectorizer(ngram_range=(1,2))
  count = CountVectorizer(ngram_range=(1,2))
  if args.vectorizer == "tfidf":
    return tfidf
  else:
    return count

def fit_model(vectorizer, X_train, y_train, X_dev, y_dev):
  """
    Train the model on the training data
  Args:
      vectorizer (Vec): CountVectorizer or TFIDF vec to convert data to numbers
      X_train: Training data text samples
      y_train: Training data labels
      X_dev:  Dev data text samples
      y_dev:  Dev data labels

  Returns:
      classifier: sklearn model SVM
  """
  SVM = svm.LinearSVC(C=1.0, penalty='l2', dual=False, random_state=1)
  classifier = Pipeline([("vec", vectorizer), ("cls", SVM)])
  classifier.fit((list(X_train)+list(X_dev)), (list(y_train)+list(y_dev)) )
  return classifier

def test_model(classifier, X_test, y_test):
  """
  Get results on test set and plot confusion matrix

  Args:
      classifier (sklearn object): svm classifier
      X_test : Test set text samples
      y_test : Test set labels
  """
  print("="*50)
  print("Baseline-2: SVM scores")
  print("="*50)

  # Let the model predict the labels of the dev set
  y_pred = classifier.predict(X_test)

  # present classification report with precison, recall and F1 per label
  print(classification_report(y_test, y_pred, target_names=set(y_test), digits=4))

  # present confusion matrix of the labels
  fig, ax = plot.subplots(figsize=(8, 8))
  # plot_confusion_matrix(slf_4, X_test, y_test, normalize='true', cmap=plt.cm.Blues, ax=ax)
  plot_confusion_matrix(classifier, X_test, y_test, cmap=plot.cm.Blues, ax=ax)
  locs, labels = plot.xticks()  
  plot.setp(labels, rotation=-90)
  plot.show()


def calc_majority(y_train, y_test):
  """Calculate the majority class baseline"""
  majority_label = pd.DataFrame(y_train).value_counts().idxmax()
  label_sexist_majority = [majority_label for i in range(len(y_test))]

  print("="*50)
  print("Baseline-1: Most frequent class algorithm scores")
  print("="*50)
  print(classification_report(y_test, label_sexist_majority, 
                              zero_division=0, digits=4))

if __name__ == '__main__':
  args = create_arg_parser()
  np.random.seed(args.seed)

  print(args)
  df = pd.read_csv(args.input_file)
  df = preprocess(df)
  X_train, X_dev, X_test, y_train, y_dev, y_test = split_data(df, args.seed)
  vectorizer = get_vectorizer(args)
  classifier = fit_model(vectorizer, X_train, y_train, X_dev, y_dev)
  calc_majority(y_train, y_test)
  test_model(classifier, X_test, y_test)